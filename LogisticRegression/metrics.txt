For binary classification:

	               | Predicted Spam	     |   Predicted Not Spam
-------------------|---------------------|------------------------
Actual Spam	       |TP (True Positive)	 |  FN (False Negative)
-------------------|---------------------|-------------------------
Actual Not Spam	   |FP (False Positive)	 |  TN (True Negative)
-------------------------------------------------------------------

TP: spam correctly detected

TN: normal message correctly allowed

FP: normal message marked as spam âŒ

FN: spam allowed through âŒ

All metrics come from this table.

2ï¸âƒ£ Accuracy

â€œHow many predictions were correct overall?â€

Formula:
Accuracy = (TP + TN) / (TP + TN + FP + FN)

Rule:

âœ… Good when classes are balanced

âŒ Misleading when data is imbalanced

ğŸ“Œ In spam detection â†’ donâ€™t trust accuracy alone

3ï¸âƒ£ Precision

â€œWhen the model predicts spam, how often is it right?â€

Formula:
Precision = TP / (TP + FP)

Rule:

High precision â†’ few false alarms

Important when FP is costly

ğŸ“Œ Example:

Email marked spam but is important â†’ BAD
4ï¸âƒ£ Recall (Sensitivity)

â€œHow much spam did the model catch?â€

Formula:
Recall = TP / (TP + FN)

Rule:

High recall â†’ few missed spam

Important when FN is costly

ğŸ“Œ Example:

Spam reaches inbox â†’ BAD

6ï¸âƒ£ F1-Score

Balance between precision and recall

Formula:
F1 = 2 * (Precision * Recall) / (Precision + Recall)

Rule:

Best single metric for imbalanced data

Penalizes extreme values

ğŸ“Œ If one is low â†’ F1 is low

7ï¸âƒ£ Support

Number of real samples per class

Rule:

Shows class imbalance

Low support â†’ metrics are less reliable

ğŸ“Œ Always check support first

8ï¸âƒ£ Macro Average

Treats all classes equally

Formula:
Macro = average(metric for each class)

Rule:

Best to judge fairness across classes

Not affected by imbalance

ğŸ“Œ Use macro F1 when classes are imbalanced

8ï¸âƒ£ Macro Average

Treats all classes equally
Macro = average(metric for each class)
Rule:
Best to judge fairness across classes

Not affected by imbalance

ğŸ“Œ Use macro F1 when classes are imbalanced

9ï¸âƒ£ Weighted Average
Weights metrics by number of samples

Rule:
Dominated by majority class

Can hide poor minority performance

ğŸ“Œ Looks good, but can be misleading

1ï¸âƒ£2ï¸âƒ£ Interpreting YOUR results (example)

From your model:

Accuracy â‰ˆ 96% â†’ looks good

Spam Recall â‰ˆ 95% â†’ excellent

Not-spam Precision â‰ˆ 68% â†’ warning âš ï¸

Macro F1 â‰ˆ 0.89 â†’ honest score